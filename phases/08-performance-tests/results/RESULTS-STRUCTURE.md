# Performance Test Results Directory Structure

> **Purpose**: Organize all test artifacts in a consistent, analyzable structure supporting multiple runs

## ðŸ“ Directory Organization

Test results are organized by test type, then by timestamp for each run:

```
results/
â”œâ”€â”€ 01-baseline-100tps/
â”‚   â”œâ”€â”€ 2024-01-15-14-30-22/
â”‚   â”œâ”€â”€ 2024-01-15-16-45-10/
â”‚   â””â”€â”€ 2024-01-16-09-15-33/
â”œâ”€â”€ 02-scale-500tps/
â”‚   â”œâ”€â”€ 2024-01-15-15-30-45/
â”‚   â””â”€â”€ 2024-01-16-10-20-15/
â”œâ”€â”€ 03-target-1000tps/
â”‚   â”œâ”€â”€ 2024-01-15-16-35-12/
â”‚   â”œâ”€â”€ 2024-01-16-11-30-22/  # First attempt
â”‚   â”œâ”€â”€ 2024-01-16-14-15-45/  # After tuning
â”‚   â””â”€â”€ 2024-01-17-09-45-18/  # Final successful run
â”œâ”€â”€ 04-endurance-1000tps/
â”‚   â””â”€â”€ 2024-01-17-14-30-00/
â”œâ”€â”€ 05-stress-to-failure/
â”‚   â””â”€â”€ 2024-01-17-17-45-30/
â”œâ”€â”€ aggregated/                 # Cross-test analysis
â””â”€â”€ RESULTS-STRUCTURE.md       # This file
```

## ðŸ—‚ï¸ Test Run Structure

Each timestamped run directory contains:

### 1. K6 Raw Data (`k6-raw-data/`)
Raw, unprocessed K6 output for detailed analysis:
- `k6-output.log` - Complete stdout/stderr from K6
- `k6-metrics.json` - All metrics in JSON format
- `k6-thresholds.json` - Pass/fail threshold results
- `k6-iterations.csv` - Per-iteration data (if enabled)

### 2. K6 Summaries (`k6-summaries/`)
Processed summaries for quick review:
- `summary.txt` - Human-readable test summary
- `summary.json` - Machine-readable summary
- `checks.json` - All check results with pass rates
- `end-of-test.json` - Final test state

### 3. Metrics (`metrics/`)
Time-series data for trending:
```
metrics/
â”œâ”€â”€ prometheus/           # Prometheus exports
â”‚   â”œâ”€â”€ mojaloop_transfers_total.json
â”‚   â”œâ”€â”€ mojaloop_transfers_duration.json
â”‚   â”œâ”€â”€ k6_http_reqs_total.json
â”‚   â””â”€â”€ k6_vus.json
â””â”€â”€ service-level/       # Per-service breakdowns
    â”œâ”€â”€ ml-api-adapter.json
    â”œâ”€â”€ central-ledger.json
    â”œâ”€â”€ account-lookup.json
    â””â”€â”€ quote-service.json
```

### 4. Resource Utilization (`resource-utilization/`)
Infrastructure metrics during test:
```
resource-utilization/
â”œâ”€â”€ cpu/
â”‚   â”œâ”€â”€ nodes.json         # CPU % per node over time
â”‚   â”œâ”€â”€ services.json      # CPU per service
â”‚   â””â”€â”€ top-consumers.json # Highest CPU users
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ nodes.json         # Memory usage per node
â”‚   â”œâ”€â”€ services.json      # Memory per service
â”‚   â””â”€â”€ oom-events.json    # OOM kill events
â”œâ”€â”€ network/
â”‚   â”œâ”€â”€ throughput.json    # Mbps in/out
â”‚   â”œâ”€â”€ connections.json   # Connection counts
â”‚   â””â”€â”€ latency.json       # Network latency
â””â”€â”€ disk/
    â”œâ”€â”€ iops.json          # Read/write IOPS
    â”œâ”€â”€ throughput.json    # MB/s throughput
    â””â”€â”€ queue-depth.json   # Disk queue metrics
```

### 5. Grafana Dashboards (`grafana-dashboards/`)
Visual evidence of performance:
```
grafana-dashboards/
â”œâ”€â”€ performance-overview/
â”‚   â”œâ”€â”€ 01-test-start.png      # Initial state
â”‚   â”œâ”€â”€ 02-ramp-up.png         # During ramp
â”‚   â”œâ”€â”€ 03-peak-tps.png        # Peak achievement
â”‚   â”œâ”€â”€ 04-steady-state.png    # Sustained performance
â”‚   â””â”€â”€ 05-test-end.png        # Final state
â”œâ”€â”€ service-health/
â”‚   â”œâ”€â”€ ml-api-adapter-peak.png
â”‚   â”œâ”€â”€ central-ledger-peak.png
â”‚   â”œâ”€â”€ account-lookup-peak.png
â”‚   â””â”€â”€ all-services-1000tps.png
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ cpu-utilization-timeline.png
â”‚   â”œâ”€â”€ memory-usage-timeline.png
â”‚   â”œâ”€â”€ network-traffic-peak.png
â”‚   â””â”€â”€ node-pressure.png
â”œâ”€â”€ k6-metrics/
â”‚   â”œâ”€â”€ vus-timeline.png       # Virtual users
â”‚   â”œâ”€â”€ request-rate.png       # Requests/sec
â”‚   â”œâ”€â”€ response-time.png      # Latency distribution
â”‚   â””â”€â”€ error-rate.png         # Errors over time
â””â”€â”€ custom/                     # Special captures
    â”œâ”€â”€ interesting-anomaly.png
    â””â”€â”€ bottleneck-evidence.png
```

### 6. Logs (`logs/`)
Detailed logs for debugging:
```
logs/
â”œâ”€â”€ k6/
â”‚   â”œâ”€â”€ runner.log         # K6 runner logs
â”‚   â””â”€â”€ coordinator.log    # If using distributed K6
â”œâ”€â”€ mojaloop-services/
â”‚   â”œâ”€â”€ ml-api-adapter.log
â”‚   â”œâ”€â”€ central-ledger.log
â”‚   â””â”€â”€ errors-only.log    # Filtered error logs
â””â”€â”€ infrastructure/
    â”œâ”€â”€ node-events.log    # Kubernetes node events
    â””â”€â”€ pod-events.log     # Pod scheduling/failures
```

### 7. Analysis (`analysis/`)
Post-test analysis outputs:
```
analysis/
â”œâ”€â”€ bottlenecks/
â”‚   â”œâ”€â”€ identified-bottlenecks.json
â”‚   â”œâ”€â”€ bottleneck-timeline.png
â”‚   â””â”€â”€ recommendations.md
â”œâ”€â”€ errors/
â”‚   â”œâ”€â”€ error-classification.json
â”‚   â”œâ”€â”€ error-timeline.png
â”‚   â””â”€â”€ root-cause-analysis.md
â””â”€â”€ trends/
    â”œâ”€â”€ latency-trend.json
    â”œâ”€â”€ throughput-trend.json
    â””â”€â”€ resource-trend.json
```

### 8. Configuration Snapshots (`config-snapshots/`)
Configuration at test time:
- `k6-test.yaml` - K6 test configuration
- `mojaloop-values.yaml` - Helm values used
- `infrastructure.json` - Node types, counts
- `service-replicas.json` - Scaling configuration
- `resource-limits.json` - Resource constraints

## ðŸ“Š Aggregated Results

The `aggregated/` directory contains cross-test analysis:

```
aggregated/
â”œâ”€â”€ comparison/
â”‚   â”œâ”€â”€ tps-progression.json      # TPS across all tests
â”‚   â”œâ”€â”€ latency-comparison.json   # Latency trends
â”‚   â”œâ”€â”€ resource-efficiency.json  # Resource per TPS
â”‚   â””â”€â”€ cost-analysis.json        # Cost per million txns
â””â”€â”€ reports/
    â”œâ”€â”€ executive-summary.pdf     # High-level results
    â”œâ”€â”€ technical-report.pdf      # Detailed analysis
    â””â”€â”€ tuning-insights.md        # Lessons learned
```

## ðŸ”§ Usage

### During Test Execution
```bash
# Create timestamped directory for this run
export TEST_TYPE="03-target-1000tps"
export TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)
export TEST_DIR="results/$TEST_TYPE/$TIMESTAMP"
mkdir -p "$TEST_DIR"/{k6-raw-data,k6-summaries,metrics,resource-utilization,grafana-dashboards,logs,analysis,config-snapshots}

# Capture K6 output
kubectl logs -f k6-target-test > "$TEST_DIR/k6-raw-data/k6-output.log"

# Export K6 metrics
kubectl exec k6-target-test -- k6 stats export > "$TEST_DIR/k6-raw-data/k6-metrics.json"

# Take dashboard screenshots and save to:
# $TEST_DIR/grafana-dashboards/performance-overview-peak-tps.png
# $TEST_DIR/grafana-dashboards/service-health-1000tps.png
# $TEST_DIR/grafana-dashboards/resource-utilization.png
```

### Manual Data Collection
```bash
# K6 Summary
kubectl logs k6-target-test --tail=200 > "$TEST_DIR/k6-summaries/summary.txt"

# Resource snapshots
kubectl top nodes > "$TEST_DIR/resource-utilization/nodes-snapshot.txt"
kubectl top pods -n mojaloop > "$TEST_DIR/resource-utilization/pods-snapshot.txt"

# Service logs
kubectl logs -n mojaloop -l app=ml-api-adapter --tail=1000 > "$TEST_DIR/logs/ml-api-adapter.log"

# Configuration snapshots
helm get values mojaloop -n mojaloop > "$TEST_DIR/config-snapshots/mojaloop-values.yaml"
```

## ðŸ“‹ Checklist for Complete Results

For each test, ensure you have:

- [ ] K6 raw output log
- [ ] K6 metrics JSON export
- [ ] K6 summary (text and JSON)
- [ ] Prometheus metrics for test duration
- [ ] Service-level metrics breakdown
- [ ] Resource utilization data (CPU, memory, network, disk)
- [ ] Grafana screenshots at key moments:
  - [ ] Test start
  - [ ] Peak TPS achievement
  - [ ] Steady state
  - [ ] Test end
- [ ] Service logs (at least last 30 minutes)
- [ ] Configuration snapshots
- [ ] Initial analysis outputs

## ðŸŽ¯ Best Practices

1. **Immediate Capture**: Take screenshots as soon as important moments occur
2. **Descriptive Names**: Use clear filenames like `1000tps-achieved-1547.png`
3. **Time Alignment**: Note exact timestamps for correlating data
4. **Raw Data First**: Always capture raw data; process later
5. **Automation**: Use scripts to avoid missing data during critical moments

## ðŸ”„ Data Retention

- **Raw Data**: Keep for 30 days minimum
- **Summaries**: Keep indefinitely
- **Screenshots**: Keep best examples indefinitely
- **Aggregated Reports**: Keep indefinitely

## ðŸ“¤ Sharing Results

To package results for sharing:
```bash
# Package single test
tar -czf target-1000tps-results.tar.gz results/03-target-1000tps/

# Package all results
tar -czf mojaloop-perf-results.tar.gz results/
```